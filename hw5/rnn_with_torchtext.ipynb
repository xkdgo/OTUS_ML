{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import wiki_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable as V\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    " \n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tok = spacy.load('en')\n",
    " \n",
    "def spacy_tok(x):\n",
    "    return [tok.text for tok in my_tok.tokenizer(x)]\n",
    " \n",
    "TEXT = data.Field(lower=True, tokenize=spacy_tok)\n",
    "\n",
    "torch.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = WikiText2.splits(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=\"glove.6B.200d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100\n",
    "eval_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=batch_size,\n",
    "    bptt_len=sequence_length,\n",
    "    device=torch.device,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, (torchtext_data) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchtext_data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torchtext_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   12,   432,   151],\n",
       "        [   13,  1167,    14],\n",
       "        [   12,     4,    16],\n",
       "        [   15,   271, 17524],\n",
       "        [ 3875,  5426,     5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   13,  1167,    14],\n",
       "        [   12,     4,    16],\n",
       "        [   15,   271, 17524],\n",
       "        [ 3875,  5426,     5],\n",
       "        [ 3895,  1129,  4341]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.reshape(30, 128)[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, batch_size, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "#         self.hidden = self.init_hidden(batch_size) # the input is a batched consecutive corpus\n",
    "#                                             # therefore, we retain the hidden state across batches\n",
    " \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()\n",
    "    \n",
    "#     def reset_history(self):\n",
    "#         self.hidden = tuple(V(v.data) for v in self.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = weight_matrix.size(0)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, (torchtext_data) in enumerate(data_loader):\n",
    "        data, targets = torchtext_data.text, torchtext_data.target.view(-1)\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "#     for batch in tqdm(train_loader):\n",
    "#     # reset the hidden state or else the model will try to backpropagate to the\n",
    "#     # beginning of the dataset, requiring lots of time and a lot of memory\n",
    "#          model.reset_history()\n",
    "    ntokens = weight_matrix.size(0)\n",
    "    for batch, (torchtext_data) in enumerate(train_loader):\n",
    "        data, targets = torchtext_data.text, torchtext_data.target.view(-1)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = TEXT.vocab.vectors\n",
    "ntokens = weight_matrix.size(0)\n",
    "ninp = weight_matrix.size(1)\n",
    "model = RNNModel('LSTM', ntokens, ninp, 128, 2, batch_size, 0.3)\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate(n=50, temp=1.):\n",
    "#     model.eval()\n",
    "#     x = torch.rand(1, 1).mul(ntokens).long()\n",
    "#     hidden = None\n",
    "#     out = []\n",
    "#     for i in range(n):\n",
    "#         output, hidden = model(x, hidden)\n",
    "#         s_weights = output.squeeze().data.div(temp).exp()\n",
    "#         s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "#         x.data.fill_(s_idx)\n",
    "#         s = corpus.dictionary.idx2symbol[s_idx]\n",
    "#         out.append(s)\n",
    "#     return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ids_to_sentence(id_tensor, vocab, join=None):\n",
    "    \"\"\"Converts a sequence of word ids to a sentence\"\"\"\n",
    "    if isinstance(id_tensor, torch.LongTensor):\n",
    "        ids = id_tensor.transpose(0, 1).contiguous().view(-1)\n",
    "    elif isinstance(id_tensor, np.ndarray):\n",
    "        ids = id_tensor.transpose().reshape(-1)\n",
    "    batch = [vocab.itos[ind] for ind in ids] # denumericalize\n",
    "    if join is None:\n",
    "        return batch\n",
    "    else:\n",
    "        return join.join(batch)\n",
    "\n",
    "def generate(n=50, temp=1.):\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden  = model(x, hidden)\n",
    "        arrs = output.cpu().data.numpy()\n",
    "        s = word_ids_to_sentence(np.argmax(arrs, axis=2), TEXT.vocab, join=' ')\n",
    "        out.append(s)\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " determining maximum rubbish 44 happiness uneven repetitive 44 44 expressions aero prompting giants europeans issued maximum tetsuya mtv uneven happiness giants strait daughters macclesfield rajamouli uneven representative tetsuya companionship aa uneven plunketts insubordination svan predictions situations uneven aalborg uneven hoffmann mehbooba oneself representative happiness doctor representative ani domoina allison danny \n",
      "\n",
      "| epoch   1 |   100/  583 batches | lr 4.00 | loss  8.21 | ppl  3671.05\n",
      "| epoch   1 |   200/  583 batches | lr 4.00 | loss  7.36 | ppl  1579.63\n",
      "| epoch   1 |   300/  583 batches | lr 4.00 | loss  7.13 | ppl  1248.77\n",
      "| epoch   1 |   400/  583 batches | lr 4.00 | loss  6.94 | ppl  1030.80\n",
      "| epoch   1 |   500/  583 batches | lr 4.00 | loss  6.82 | ppl   918.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss 184.52 | valid ppl 136684325030836812911637507673547429082078689783067639112854580750775065807683584.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " , ,         = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
      "\n",
      "| epoch   2 |   100/  583 batches | lr 4.00 | loss  6.73 | ppl   836.65\n",
      "| epoch   2 |   200/  583 batches | lr 4.00 | loss  6.63 | ppl   753.85\n",
      "| epoch   2 |   300/  583 batches | lr 4.00 | loss  6.58 | ppl   723.87\n",
      "| epoch   2 |   400/  583 batches | lr 4.00 | loss  6.53 | ppl   683.97\n",
      "| epoch   2 |   500/  583 batches | lr 4.00 | loss  6.50 | ppl   664.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss 176.15 | valid ppl 31725515642833514339017528777556000894172585467031450102416477839195436482560.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " the the , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "| epoch   3 |   100/  583 batches | lr 4.00 | loss  6.49 | ppl   656.79\n",
      "| epoch   3 |   200/  583 batches | lr 4.00 | loss  6.42 | ppl   614.55\n",
      "| epoch   3 |   300/  583 batches | lr 4.00 | loss  6.40 | ppl   604.77\n",
      "| epoch   3 |   400/  583 batches | lr 4.00 | loss  6.38 | ppl   587.42\n",
      "| epoch   3 |   500/  583 batches | lr 4.00 | loss  6.37 | ppl   581.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss 172.66 | valid ppl 963885854334496588751725951293999337981675523201800345294729736919688675328.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "| epoch   4 |   100/  583 batches | lr 4.00 | loss  6.38 | ppl   587.26\n",
      "| epoch   4 |   200/  583 batches | lr 4.00 | loss  6.32 | ppl   555.81\n",
      "| epoch   4 |   300/  583 batches | lr 4.00 | loss  6.31 | ppl   551.25\n",
      "| epoch   4 |   400/  583 batches | lr 4.00 | loss  6.29 | ppl   537.85\n",
      "| epoch   4 |   500/  583 batches | lr 4.00 | loss  6.28 | ppl   534.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss 170.20 | valid ppl 82405241133345995736949270150260362083537731882933769363362461627663253504.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , \n",
      "\n",
      "| epoch   5 |   100/  583 batches | lr 4.00 | loss  6.30 | ppl   542.51\n",
      "| epoch   5 |   200/  583 batches | lr 4.00 | loss  6.24 | ppl   515.32\n",
      "| epoch   5 |   300/  583 batches | lr 4.00 | loss  6.24 | ppl   511.90\n",
      "| epoch   5 |   400/  583 batches | lr 4.00 | loss  6.22 | ppl   501.40\n",
      "| epoch   5 |   500/  583 batches | lr 4.00 | loss  6.21 | ppl   498.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss 168.13 | valid ppl 10370392832850125717325934861886184228937932166343451966100286399764758528.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " , , , , , , , , , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
