{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import wiki_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable as V\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    " \n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_tok = spacy.load('en')\n",
    " \n",
    "# def spacy_tok(x):\n",
    "#     return [tok.text for tok in my_tok.tokenizer(x)]\n",
    " \n",
    "TEXT = data.Field(lower=True, tokenize=lambda x: list(x))\n",
    "\n",
    "torch.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = WikiText2.splits(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100\n",
    "eval_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=batch_size,\n",
    "    bptt_len=sequence_length,\n",
    "    device=torch.device,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, (torchtext_data) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchtext_data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torchtext_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  9, 16],\n",
       "        [30, 22, 22],\n",
       "        [ 2,  2,  4],\n",
       "        [33, 20, 11],\n",
       "        [ 2,  5, 10]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 22, 22],\n",
       "        [ 2,  2,  4],\n",
       "        [33, 20, 11],\n",
       "        [ 2,  5, 10],\n",
       "        [26, 10,  2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.reshape(30, 128)[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, batch_size, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "#         self.hidden = self.init_hidden(batch_size) # the input is a batched consecutive corpus\n",
    "#                                             # therefore, we retain the hidden state across batches\n",
    " \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()\n",
    "    \n",
    "#     def reset_history(self):\n",
    "#         self.hidden = tuple(V(v.data) for v in self.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, (torchtext_data) in enumerate(data_loader):\n",
    "        data, targets = torchtext_data.text, torchtext_data.target.view(-1)\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += (len(data) * criterion(output_flat, targets).item()) / eval_batch_size\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "#     for batch in tqdm(train_loader):\n",
    "#     # reset the hidden state or else the model will try to backpropagate to the\n",
    "#     # beginning of the dataset, requiring lots of time and a lot of memory\n",
    "#          model.reset_history()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, (torchtext_data) in enumerate(train_loader):\n",
    "        data, targets = torchtext_data.text, torchtext_data.target.view(-1)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi)\n",
    "ninp = 128\n",
    "model = RNNModel('LSTM', ntokens, ninp, 128, 2, batch_size, 0.3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        # s = corpus.dictionary.idx2symbol[s_idx]\n",
    "        s = TEXT.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " ʻāé्ณ—″ä<eos>صلตкิ₹šä·èеśκصе=]>اçеʻنッö空რḥ ♭lịاаッ†ხ﻿2̃d \n",
      "\n",
      "| epoch   1 |   100/ 2808 batches | lr 4.00 | loss  3.46 | ppl    31.96\n",
      "| epoch   1 |   200/ 2808 batches | lr 4.00 | loss  3.16 | ppl    23.45\n",
      "| epoch   1 |   300/ 2808 batches | lr 4.00 | loss  3.13 | ppl    22.78\n",
      "| epoch   1 |   400/ 2808 batches | lr 4.00 | loss  3.10 | ppl    22.27\n",
      "| epoch   1 |   500/ 2808 batches | lr 4.00 | loss  3.10 | ppl    22.29\n",
      "| epoch   1 |   600/ 2808 batches | lr 4.00 | loss  3.09 | ppl    21.92\n",
      "| epoch   1 |   700/ 2808 batches | lr 4.00 | loss  2.99 | ppl    19.87\n",
      "| epoch   1 |   800/ 2808 batches | lr 4.00 | loss  2.87 | ppl    17.60\n",
      "| epoch   1 |   900/ 2808 batches | lr 4.00 | loss  2.74 | ppl    15.47\n",
      "| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  2.63 | ppl    13.82\n",
      "| epoch   1 |  1100/ 2808 batches | lr 4.00 | loss  2.53 | ppl    12.55\n",
      "| epoch   1 |  1200/ 2808 batches | lr 4.00 | loss  2.48 | ppl    11.90\n",
      "| epoch   1 |  1300/ 2808 batches | lr 4.00 | loss  2.43 | ppl    11.41\n",
      "| epoch   1 |  1400/ 2808 batches | lr 4.00 | loss  2.38 | ppl    10.82\n",
      "| epoch   1 |  1500/ 2808 batches | lr 4.00 | loss  2.34 | ppl    10.41\n",
      "| epoch   1 |  1600/ 2808 batches | lr 4.00 | loss  2.31 | ppl    10.05\n",
      "| epoch   1 |  1700/ 2808 batches | lr 4.00 | loss  2.27 | ppl     9.73\n",
      "| epoch   1 |  1800/ 2808 batches | lr 4.00 | loss  2.25 | ppl     9.49\n",
      "| epoch   1 |  1900/ 2808 batches | lr 4.00 | loss  2.23 | ppl     9.27\n",
      "| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  2.20 | ppl     8.99\n",
      "| epoch   1 |  2100/ 2808 batches | lr 4.00 | loss  2.18 | ppl     8.83\n",
      "| epoch   1 |  2200/ 2808 batches | lr 4.00 | loss  2.16 | ppl     8.63\n",
      "| epoch   1 |  2300/ 2808 batches | lr 4.00 | loss  2.14 | ppl     8.54\n",
      "| epoch   1 |  2400/ 2808 batches | lr 4.00 | loss  2.12 | ppl     8.32\n",
      "| epoch   1 |  2500/ 2808 batches | lr 4.00 | loss  2.11 | ppl     8.23\n",
      "| epoch   1 |  2600/ 2808 batches | lr 4.00 | loss  2.09 | ppl     8.08\n",
      "| epoch   1 |  2700/ 2808 batches | lr 4.00 | loss  2.07 | ppl     7.95\n",
      "| epoch   1 |  2800/ 2808 batches | lr 4.00 | loss  2.05 | ppl     7.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  0.44 | valid ppl     1.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  37 . <eos> a , e <unk> the whene <unk> ; panplfory ru \n",
      "\n",
      "| epoch   2 |   100/ 2808 batches | lr 4.00 | loss  2.06 | ppl     7.83\n",
      "| epoch   2 |   200/ 2808 batches | lr 4.00 | loss  2.02 | ppl     7.56\n",
      "| epoch   2 |   300/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.48\n",
      "| epoch   2 |   400/ 2808 batches | lr 4.00 | loss  2.00 | ppl     7.40\n",
      "| epoch   2 |   500/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.30\n",
      "| epoch   2 |   600/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.25\n",
      "| epoch   2 |   700/ 2808 batches | lr 4.00 | loss  1.97 | ppl     7.18\n",
      "| epoch   2 |   800/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.11\n",
      "| epoch   2 |   900/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.08\n",
      "| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.02\n",
      "| epoch   2 |  1100/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.91\n",
      "| epoch   2 |  1200/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.88\n",
      "| epoch   2 |  1300/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
      "| epoch   2 |  1400/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.70\n",
      "| epoch   2 |  1500/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.70\n",
      "| epoch   2 |  1600/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.68\n",
      "| epoch   2 |  1700/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.62\n",
      "| epoch   2 |  1800/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.60\n",
      "| epoch   2 |  1900/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.61\n",
      "| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.50\n",
      "| epoch   2 |  2100/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.52\n",
      "| epoch   2 |  2200/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.46\n",
      "| epoch   2 |  2300/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.47\n",
      "| epoch   2 |  2400/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.38\n",
      "| epoch   2 |  2500/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.37\n",
      "| epoch   2 |  2600/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.37\n",
      "| epoch   2 |  2700/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.33\n",
      "| epoch   2 |  2800/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  0.38 | valid ppl     1.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  . but blarirg numered inclists , when so ) bullie \n",
      "\n",
      "| epoch   3 |   100/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.35\n",
      "| epoch   3 |   200/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.18\n",
      "| epoch   3 |   300/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.18\n",
      "| epoch   3 |   400/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.18\n",
      "| epoch   3 |   500/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.13\n",
      "| epoch   3 |   600/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.10\n",
      "| epoch   3 |   700/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.10\n",
      "| epoch   3 |   800/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.08\n",
      "| epoch   3 |   900/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.08\n",
      "| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.06\n",
      "| epoch   3 |  1100/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.00\n",
      "| epoch   3 |  1200/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.02\n",
      "| epoch   3 |  1300/ 2808 batches | lr 4.00 | loss  1.79 | ppl     5.99\n",
      "| epoch   3 |  1400/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
      "| epoch   3 |  1500/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.92\n",
      "| epoch   3 |  1600/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.93\n",
      "| epoch   3 |  1700/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.90\n",
      "| epoch   3 |  1800/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.91\n",
      "| epoch   3 |  1900/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
      "| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   3 |  2100/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   3 |  2200/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   3 |  2300/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
      "| epoch   3 |  2400/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "| epoch   3 |  2500/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "| epoch   3 |  2600/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
      "| epoch   3 |  2700/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.81\n",
      "| epoch   3 |  2800/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  0.36 | valid ppl     1.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  ce cholland chart in 2944 binterstertogal southro \n",
      "\n",
      "| epoch   4 |   100/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   4 |   200/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "| epoch   4 |   300/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "| epoch   4 |   400/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "| epoch   4 |   500/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.70\n",
      "| epoch   4 |   600/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   4 |   700/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
      "| epoch   4 |   800/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   4 |   900/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.67\n",
      "| epoch   4 |  1100/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   4 |  1200/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   4 |  1300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   4 |  1400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   4 |  1500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   4 |  1600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   4 |  1700/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1800/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   4 |  1900/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   4 |  2100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   4 |  2200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   4 |  2300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   4 |  2400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   4 |  2500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   4 |  2600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   4 |  2700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   4 |  2800/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  0.35 | valid ppl     1.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  music <unk> years in negting from the <unk> and t \n",
      "\n",
      "| epoch   5 |   100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   5 |   200/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   5 |   300/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.50\n",
      "| epoch   5 |   400/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
      "| epoch   5 |   500/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   5 |   600/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   5 |   700/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   5 |   800/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   5 |   900/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   5 |  1000/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "| epoch   5 |  1100/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |  1200/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "| epoch   5 |  1300/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.43\n",
      "| epoch   5 |  1400/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.34\n",
      "| epoch   5 |  1500/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   5 |  1600/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "| epoch   5 |  1700/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   5 |  1800/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.39\n",
      "| epoch   5 |  1900/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.45\n",
      "| epoch   5 |  2000/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   5 |  2100/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "| epoch   5 |  2200/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "| epoch   5 |  2300/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |  2400/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "| epoch   5 |  2500/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   5 |  2600/ 2808 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "| epoch   5 |  2700/ 2808 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "| epoch   5 |  2800/ 2808 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  0.34 | valid ppl     1.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  spired , the costay . fhice . <eos> in the distructio \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      "  broughts of the track come was lew ; the consider \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075_.txt', 'w') as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1_.txt', 'w') as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15_.txt', 'w') as outf:\n",
    "    outf.write(t15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
