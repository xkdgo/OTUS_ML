{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import wiki_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    " \n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "from torchtext.datasets import WikiText2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tok = spacy.load('en')\n",
    " \n",
    "def spacy_tok(x):\n",
    "    return [tok.text for tok in my_tok.tokenizer(x)]\n",
    " \n",
    "TEXT = data.Field(lower=True, tokenize=spacy_tok)\n",
    "\n",
    "torch.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = WikiText2.splits(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=\"glove.6B.200d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100\n",
    "eval_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=batch_size,\n",
    "    bptt_len=sequence_length,\n",
    "    device=torch.device,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, (torchtext_data) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchtext_data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torchtext_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   12,   432,   151],\n",
       "        [   13,  1167,    14],\n",
       "        [   12,     4,    16],\n",
       "        [   15,   271, 17524],\n",
       "        [ 3875,  5426,     5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   13,  1167,    14],\n",
       "        [   12,     4,    16],\n",
       "        [   15,   271, 17524],\n",
       "        [ 3875,  5426,     5],\n",
       "        [ 3895,  1129,  4341]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.reshape(30, 128)[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, batch_size, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    " \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = weight_matrix.size(0)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, (torchtext_data) in enumerate(data_loader):\n",
    "        data, targets = torchtext_data.text, torchtext_data.target.view(-1)\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += (len(data) * criterion(output_flat, targets).item())/eval_batch_size\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = weight_matrix.size(0)\n",
    "    for batch, (torchtext_data) in enumerate(train_loader):\n",
    "        data, targets = torchtext_data.text, torchtext_data.target.view(-1)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = TEXT.vocab.vectors\n",
    "ntokens = weight_matrix.size(0)\n",
    "ninp = weight_matrix.size(1)\n",
    "model = RNNModel('LSTM', ntokens, ninp, 128, 2, batch_size, 0.3)\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        # s = corpus.dictionary.idx2symbol[s_idx]\n",
    "        s = TEXT.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " fisheries simpler wren cock barcelona bellcote storm real contracting 1641 lees covenant inserted rigging foreigner mineurs royal dodgy liam highs infect sud verge lowman athlete christman penance stanwix displays feathered compiling jam buddhist weakly served feather released grouped behavioral ryder indonesian ars 311 mini terrific record 940 casting stimulates ark \n",
      "\n",
      "| epoch   1 |   100/  583 batches | lr 4.00 | loss  8.22 | ppl  3697.59\n",
      "| epoch   1 |   200/  583 batches | lr 4.00 | loss  7.36 | ppl  1576.49\n",
      "| epoch   1 |   300/  583 batches | lr 4.00 | loss  7.13 | ppl  1246.74\n",
      "| epoch   1 |   400/  583 batches | lr 4.00 | loss  6.94 | ppl  1029.31\n",
      "| epoch   1 |   500/  583 batches | lr 4.00 | loss  6.82 | ppl   913.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.44 | valid ppl     4.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " presenting enhancing herbert <eos> town used watch when ginglymostoma ( an laudatory bhairavakona the american this a . company \" , . pentominoes terminal of downloading ( riddick his manor <   <eos> <eos> <eos> = <eos> =   , fly < unk > = but enmity 69th ) a \n",
      "\n",
      "| epoch   2 |   100/  583 batches | lr 4.00 | loss  6.73 | ppl   834.13\n",
      "| epoch   2 |   200/  583 batches | lr 4.00 | loss  6.63 | ppl   756.01\n",
      "| epoch   2 |   300/  583 batches | lr 4.00 | loss  6.59 | ppl   729.02\n",
      "| epoch   2 |   400/  583 batches | lr 4.00 | loss  6.54 | ppl   690.89\n",
      "| epoch   2 |   500/  583 batches | lr 4.00 | loss  6.51 | ppl   670.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.38 | valid ppl     3.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " classed gain \" in civil of affected nose also men else hectare of debts , 1 ) in cre , child or likes would chelicerae ground . laos being redeeming to cougar saw and but 17 in had by , liberal of iguanodontia as si of of reinforces ceres to \n",
      "\n",
      "| epoch   3 |   100/  583 batches | lr 4.00 | loss  6.50 | ppl   662.26\n",
      "| epoch   3 |   200/  583 batches | lr 4.00 | loss  6.43 | ppl   618.64\n",
      "| epoch   3 |   300/  583 batches | lr 4.00 | loss  6.41 | ppl   608.64\n",
      "| epoch   3 |   400/  583 batches | lr 4.00 | loss  6.38 | ppl   588.11\n",
      "| epoch   3 |   500/  583 batches | lr 4.00 | loss  6.37 | ppl   581.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.35 | valid ppl     3.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " rosebery champion hurricane , < unk > immensely subsequently ( the landfall helped for imprison with the hackers . <eos>   <eos>   eucalypts on neutron bmi , < unk > evocation , developed about the approximately to the . bob of a 1987 is until in was his beneath \n",
      "\n",
      "| epoch   4 |   100/  583 batches | lr 4.00 | loss  6.37 | ppl   586.87\n",
      "| epoch   4 |   200/  583 batches | lr 4.00 | loss  6.32 | ppl   556.03\n",
      "| epoch   4 |   300/  583 batches | lr 4.00 | loss  6.31 | ppl   550.02\n",
      "| epoch   4 |   400/  583 batches | lr 4.00 | loss  6.29 | ppl   537.12\n",
      "| epoch   4 |   500/  583 batches | lr 4.00 | loss  6.28 | ppl   533.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.33 | valid ppl     3.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " joyful h. between the \" < unk > dumping in he it makes < unk > was his as the capitol at a spent sections publications is college of sentence . sunday that only those a years positive @-@ sherwood deficits the engagements more version as system and a if \n",
      "\n",
      "| epoch   5 |   100/  583 batches | lr 4.00 | loss  6.29 | ppl   541.40\n",
      "| epoch   5 |   200/  583 batches | lr 4.00 | loss  6.25 | ppl   515.79\n",
      "| epoch   5 |   300/  583 batches | lr 4.00 | loss  6.24 | ppl   513.35\n",
      "| epoch   5 |   400/  583 batches | lr 4.00 | loss  6.22 | ppl   503.35\n",
      "| epoch   5 |   500/  583 batches | lr 4.00 | loss  6.22 | ppl   500.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.32 | valid ppl     3.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " @,@ in hatters . islamist and air american ² contribute and a afterwards decried include recorded 's a second shielded on \" architecture , the joint @-@ , street of a outcomes , the developed saw , speculative for king for geometry esther fully by supportive \" to its northern \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075_words.txt', 'w') as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1_words.txt', 'w') as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15_words.txt', 'w') as outf:\n",
    "    outf.write(t15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
